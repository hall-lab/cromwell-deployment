# This line is required. It pulls in default overrides from the embedded cromwell `application.conf` needed for proper
# performance of cromwell.
include required(classpath("application"))

webservice {
  port = 8000
  interface = 0.0.0.0
  binding-timeout = 5s
  instance.name = "cromwell"
}

akka {
  # Allow longer timeout
  # https://gatkforums.broadinstitute.org/gatk/discussion/comment/41714#Comment_41714
  http {
    server {
      request-timeout = 300s
      idle-timeout = 300s
    }
  }
}

system {
  # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  #abort-jobs-on-terminate = false

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  graceful-server-shutdown = true

  # If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  workflow-restart = true

  # Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 5000

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 50

  # Number of seconds between workflow launches
  new-workflow-poll-rate = 20

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  number-of-cache-read-workers = 25
  
  io {
    # Global Throttling - This is mostly useful for GCS and can be adjusted to match
    # the quota availble on the GCS API
    number-of-requests = 100000
    per = 100 seconds
    
    # Number of times an I/O operation should be attempted before giving up and failing it.
    number-of-attempts = 50
  }
  
  input-read-limits {
    # based on https://github.com/broadinstitute/cromwell/issues/2768
    lines = 500000
  }
}

workflow-options {
  workflow-log-dir: "cromwell-workflow-logs"

  workflow-log-temporary: true
  workflow-failure-mode: "ContinueWhilePossible"

  default {
    # When a workflow type is not provided on workflow submission, this specifies the default type.
    workflow-type: WDL

    # When a workflow type version is not provided on workflow submission, this specifies the default type version.
    workflow-type-version: "draft-2"
  }
}

# Optional call-caching configuration.
call-caching {
  enabled = true
  invalidate-bad-cache-results = true
}

google {

  application-name = "cromwell"

  auths = [
    {
      name = "application-default"
      scheme = "application_default"
    },
  ]
}

docker {
  hash-lookup {
    # Set this to match your available quota against the Google Container Engine API
    gcr-api-queries-per-100-seconds = 1000
    # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again
    cache-entry-ttl = "20 minutes"
    # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache
    cache-size = 200
    # How should docker hashes be looked up. Possible values are "local" and "remote"
    # "local": Lookup hashes on the local docker daemon using the cli
    # "remote": Lookup hashes on docker hub and gcr
    method = "remote"
  }
}

engine {
  # This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.
  # For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.
  # If you intend to be able to run workflows with this kind of declarations:
  # workflow {
  #    String str = read_string("gs://bucket/my-file.txt")
  # }
  # You will need to provide the engine with a gcs filesystem
  # Note that the default filesystem (local) is always available.
  # Note that we may need to set a project default for this to work with no project specified (it's unclear to me).
  filesystems {
    gcs {
      auth = "application-default"
     #project = "google-billing-project"
    }
    local {
      enabled: true      
    }
  }
}

backend {
  default = "JES"
  providers {
    JES {
      # NOTE - This is PAPIv2
      actor-factory = "cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"
      config {
        # Google project
        # TODO - We may not want to set these in the config and instead force them to be set on server instantiation or with workflow options.
        project = "washu-genome-inh-dis-analysis"
        root = "gs://wustl-ccdg-cromwell-processing/cromwell-executions"
    
        # Set this to the lower of the two values "Queries per 100 seconds" and "Queries per 100 seconds per user" for
        # your project.
        #
        # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will
        # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.
        # 1000 is the default "Queries per 100 seconds per user", 50000 is the default "Queries per 100 seconds"
        # See https://cloud.google.com/genomics/quotas for more information
        genomics-api-queries-per-100-seconds = 1000
    
        # Polling for completion backs-off gradually for slower-running jobs.
        # This is the maximum polling interval (in seconds):
        maximum-polling-interval = 600
    
        genomics {
          # A reference to an auth defined in the `google` stanza at the top.  This auth is used to create
          # Pipelines and manipulate auth JSONs.
          auth = "application-default"
    
    
          # alternative service account to use on the launched compute instance
          # NOTE: If combined with service account authorization, both that serivce account and this service account
          # must be able to read and write to the 'root' GCS path
          compute-service-account = "hall-lab-cromwell-executor@washu-genome-inh-dis-analysis.iam.gserviceaccount.com"
    
          # Endpoint for APIs, no reason to change this unless directed by Google.
          endpoint-url = "https://genomics.googleapis.com/"
        }
    
        filesystems {
          gcs {
            # A reference to a potentially different auth for manipulating files via engine functions.
            auth = "application-default"
          }
        }
    
      }
    }
  }
}

services {
  MetadataService {
    config {
      # Set this value to "Inf" to turn off metadata summary refresh.  The default value is currently "2 seconds".
      # metadata-summary-refresh-interval = "Inf"
      # For higher scale environments, e.g. many workflows and/or jobs, DB write performance for metadata events
      # can improved by writing to the database in batches. Increasing this value can dramatically improve overall
      # performance but will both lead to a higher memory usage as well as increase the risk that metadata events
      # might not have been persisted in the event of a Cromwell crash.
      #
      # For normal usage the default value of 1 (effectively no batching) should be fine but for larger/production
      # environments we recommend a value of at least 500. There'll be no one size fits all number here so we recommend
      # benchmarking performance and tuning the value to match your environment
      db-batch-size = 500
      #
      # Periodically the stored metadata events will be forcibly written to the DB regardless of if the batch size
      # has been reached. This is to prevent situations where events wind up never being written to an incomplete batch
      # with no new events being generated. The default value is currently 5 seconds
      # db-flush-rate = 5 seconds

      # For the Google PubSub MetadataService implementation: cromwell.services.metadata.impl.pubsub.PubSubMetadataServiceActor:
      #   Google project
      #   project = "my-project"
      #   The auth *must* be a service-account auth with JSON auth.
      #   auth = "service-account"
      #   The PubSub topic to write to. Will be created if it doesn't already exist. Defaults to "cromwell-metadata"
      #   topic = "cromwell-metadata"
      #   An optional PubSub subscription name. If supplied and if it doesn't already exist, it will be created and
      #   subscribed to the topic
      #   subscription = "optional-subscription"
      #   An application name to set on your PubSub interaction.
      #   appName = "cromwell"
    }
  }

  Instrumentation {
    # StatsD - Send metrics to a StatsD server over UDP
    # class = "cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActor"
    # config.statsd {
    #   hostname = "localhost"
    #   port = 8125
    #   prefix = "" # can be used to prefix all metrics with an api key for example
    #   flush-rate = 1 second # rate at which aggregated metrics will be sent to statsd
    # }
  }
  HealthMonitor {
    config {
      # How long to wait between status check sweeps
      # check-refresh-time = 5 minutes
      # For any given status check, how long to wait before assuming failure
      # check-timeout = 1 minute
      # For any given status datum, the maximum time a value will be kept before reverting back to "Unknown"
      # status-ttl = 15 minutes
      # For any given status check, how many times to retry a failure before setting status to failed. Note this
      # is the number of retries before declaring failure, not the total number of tries which is 1 more than
      # the number of retries.
      # check-failure-retry-count = 3
      # For any given status check, how long to wait between failure retries.
      # check-failure-retry-interval = 30 seconds

      ## When using the WorkbenchHealthMonitorServiceActor, the following are possibilities

      # This *MUST* be set to the name of the PAPI (aka JES) backend one defined in the Backends stanza. Most likely
      # it is "Jes" or "JES"
      # papi-backend-name = JES

      # The name of an authentication scheme to use for e.g. pinging PAPI and GCS. This should be either an application
      # default or service account auth, otherwise things won't work as there'll not be a refresh token where you need
      # them.
      # google-auth-name = application-default

      # A bucket in GCS to periodically stat to check for connectivity. This must be accessible by the auth mode
      # specified by google-auth-name
      # gcs-bucket-to-check = some-bucket-name
    }
  }
  LoadController {
    config {
      # The load controller service will periodically look at the status of various metrics its collecting and make an
      # assessment of the system's load. If necessary an alert will be sent to the rest of the system.
      # This option sets how frequently this should happen
      # To disable load control, set this to "Inf"
      # control-frequency = 5 seconds
    }
  }
}

database {
  # mysql example
  profile = "slick.jdbc.MySQLProfile$"
  db {
    driver = "com.mysql.jdbc.Driver"
    url = "jdbc:mysql://{{ cloudsql['ip'] }}:3306/cromwell?rewriteBatchedStatements=true&useSSL=false"
    user = "root"
    password = {{ cloudsql['password'] }}
    connectionTimeout = 5000
  }

  # For batch inserts the number of inserts to send to the DB at a time
  # insert-batch-size = 2000

  migration {
    # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of
    # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be
    # retrieved and processed at a time, before asking for the next chunk.
    read-batch-size = 100000

    # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single
    # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out
    # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.
    write-batch-size = 100000
  }
}
